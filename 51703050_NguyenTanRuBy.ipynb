{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "51703050_NguyenTanRuBy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhtMTj6-_EW4",
        "outputId": "5e6682cf-779a-49a9-e9e3-c444116225df"
      },
      "source": [
        "! pip install pyspark"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxMOIEKHAImR"
      },
      "source": [
        "import pyspark"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPweKTum0Uo2"
      },
      "source": [
        "import pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster('local[4]').setAppName('count word frequency')\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wxj9K5p_jmW"
      },
      "source": [
        "def readFile(path = \"data.txt\"):\n",
        "  output = sc.textFile(path)\n",
        "  return output.collect()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k_eNyaNBoRL"
      },
      "source": [
        "def countWordFreq(data):\n",
        "  rdd = sc.parallelize(data[0].split())\n",
        "  counts = rdd.map(lambda word: (word,1))\n",
        "  counts = counts.reduceByKey(lambda x,y: x+y)\n",
        "  return counts"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljae_9vfChzM"
      },
      "source": [
        "def filterMostFeq(data, k = 1):\n",
        "  data = data.map(lambda x: (x[1], x[0])).sortByKey(False).take(k)\n",
        "  return data"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrBanTA8D7p4",
        "outputId": "a07b7d8f-e381-4bcf-c150-979fe5b0890c"
      },
      "source": [
        "lines = readFile()\n",
        "data = countWordFreq(lines)\n",
        "output = filterMostFeq(data, 3)\n",
        "print(output)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(5, 'Aenean'), (5, 'eget'), (4, 'sit')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}